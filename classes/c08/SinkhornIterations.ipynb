{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be9aae1f",
   "metadata": {
    "cellId": "fkthxhw9re41t3w9y7n0lt",
    "execution_id": "4192e09b-5427-4529-be25-360e3a7afff4"
   },
   "source": [
    "# Sinkhorn iterations\n",
    "https://www.youtube.com/watch?v=BfOjrQAhG4M\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40c3301",
   "metadata": {
    "cellId": "gxqnamkp6qleduwdsc4fc"
   },
   "source": [
    "## Формулировка\n",
    "Пусть даны два набора ключевых точек (kp): набор $n$ размера N (например синий), и набор $m$ размера M (например красный).\n",
    "\n",
    "Для данной матрицы схожести ключевых точек $ S_{ij}[N, M] = (f^n_{i}, f^m_{j}) $ и данного параметра $\\alpha_d$ (dustbin) надо найти матрицу неотрицательных $P_{ij}[N+1, M+1]$ такую что:\n",
    "1. максимальна $ C = \\sum_{i,j}^{N+1, M+1} P_{ij} \\hat{S_{ij}}$, где $\\hat{S}[N+1, M+1]$ (coupling в коде) - это $S[N, M]$ расширенная(конкатенированная) строкой и столбцом, заполненными значением $\\alpha_d$  \n",
    "1. для любых i <= N выполняется: $\\sum_{j}^{M+1}P_{ij} = 1$ (каждый i-й(синий) kp из $n$ \"перетекает\" с коэффициентами $P_{ij}$ в j-е kp из $m$ и в m-dustbin)\n",
    "1. для любых j <= M выполняется: $\\sum_{i}^{N+1}P_{ij} = 1$ (каждый j-й(красный) kp из $m$ \"перетекает\" с коэффициентами $P_{ij}$ в i-е kp из $n$ и в n-dustbin)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a12b57",
   "metadata": {
    "cellId": "dovncs01a1cc0yabn28s0w"
   },
   "source": [
    "\n",
    "## Расширенная формулировка с учетом dustbin\n",
    "Пункты выше - стандартные для оптимального транспорта, но у нас есть еще dustbin-ы, которые надо учесть отдельно. Для реальных kp ограничения по перетеканию строгие - т.е. сумма вероятностей должна быть **равна** 1. Для dustbin ограничения нестрогие по смыслу : в m-dustbin может попасть от 0 до N kp, в n-dustbin может попасть от 0 до M kp. Проблема в том, что метод множителей Лагранжа предполагает, что ограничения заданы строгими равенствами (хотя вообще есть расширения метода на неравенства). Поэтому авторы, судя по коду, делают такой трюк - они задают следующие **строгие** ограничения для каждого dustbin:\n",
    "1. $\\sum_{i}^{N+1}P_{i,M+1} = N$\n",
    "1. $\\sum_{j}^{N+1}P_{N+1,j} = M$\n",
    "\n",
    "На первый взгляд кажется, что это какая-то фигня: получается что N синих kp и M красных kp должны быть отправлены в соответствующие dustbin, т.е. вообще все ключевые точки должны быть отправлены в dustbin. Но так не происходит из за последнего (правого углового) элемента матрицы $P_{N+1,M+1}$, который по смыслу не соответствует ни матчингу какой-то реальной точки с другой точкой, ни матчингу реальной точки с dustbin. Этот элемент участвует только в нормировке dustbin, потому может принимать любое значение - можно считать его численной заглушкой. После итераций его значение всегда получается равным сумме вероятностей всех сматченных точек. Можно визуализировать получающееся решение после итераций такой блок-матрицей (указаны суммы значений элементов блоков, а не сами элементы; KP - ключевые точки, DB - dustbin):\n",
    "\n",
    "KP | Kp N        || DB\n",
    "---|-------------||---\n",
    "KpM|     P       || M - P \n",
    "   |             ||\n",
    "DB |  N - P      || P\n",
    "\n",
    "Сумма всех реальных сматченных точек = P, это примерно количество матчей. Сумма всех элементов матрицы = M+N **всегда** (для любого P) - это количество kp. Сумма (количество) всех реальных несматченных точек - (N-P) и (M-P) соответственно, что логично. Сумма каждого из dustbin N и M соответственно. Вся \"структура\" матрицы определяется всего одной переменной P(количество матчей) - поэтому матрица-решение такого вида существует всегда. Например:\n",
    "1. Если дано N = M точек, и все сматчены, то P=N=M (количество матчей), а N-P=M-P=0 (количество отправленных в dustbin)\n",
    "1. Если даны N и M точек, и никто ни с кем не с матчен, то P=0, N-P=N, M-P=M\n",
    "\n",
    "В итоге довольно странные ограничения на dustbin приводят к логичному решению с более-менее интерпретируемыми свойствами. Интересно, как до этого додумались авторы, имхо это все ни разу не очевидно."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b451b9f0",
   "metadata": {
    "cellId": "ynsghdsi4dgvml0copti9a"
   },
   "source": [
    "\n",
    "\n",
    "## [Лагранжиан](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BC%D0%BD%D0%BE%D0%B6%D0%B8%D1%82%D0%B5%D0%BB%D0%B5%D0%B9_%D0%9B%D0%B0%D0%B3%D1%80%D0%B0%D0%BD%D0%B6%D0%B0)\n",
    "Лагранжиан состоит из четырех частей:\n",
    "1. Максимизируемая сумма \"схожести\"\n",
    "1. Добавка из Sinkhorn Iterations $P \\cdot log(P)$. Возьмем ее с коэффициентом 1 для простоты, могли бы умножить на 0.1 или 0.42 - влияет на \"сглаженность\" оптимального решения.\n",
    "1. Ограничения реальных матчей: \"по i\" (для синих), множители Лагранжа $l^{u}$, в коде $u = l^u$, \"по j\" (для красных), множители Лагранжа $l^{v}$, в коде $v = l^v$\n",
    "1. Ограничения для dustbin (записываем их сначала отдельно для лучшей читаемости). Множители лагранжа для них хранятся как последний элемент $l^{u}_{N+1}$ и $l^{v}_{M+1}$, аналогично тому как сделано в коде.  \n",
    "\n",
    "Получается:\n",
    "$$ L(P_{ij}[N+1, M+1], l^{u}_{i}[N+1], l^{v}_{j}[M+1]) = $$\n",
    "$$\\sum_{i,j}^{N+1, M+1} P_{ij} \\hat{S_{ij}} +   \n",
    "\\sum_{i,j}^{N+1, M+1} P_{ij} (log(P_{ij}) - 1) + \\newline\n",
    "\\sum_{i}^{N} l^{u}_{i} (\\sum_{j}^{M+1}P_{ij} - 1) + \\sum_{j}^{M} l^{v}_{j} (\\sum_{i}^{N+1}P_{ij} - 1) + \\newline\n",
    "l^{u}_{N+1} (\\sum_{j}^{M+1}P_{N+1,j} - M) + l^{v}_{M+1} (\\sum_{i}^{N+1}P_{i,M+1} - N)\n",
    "$$ \n",
    "\n",
    "Для удобства запишем две последних части в одну: обозначим как $\\mu[N+1]$ и $\\nu[M+1]$ требуемые значения сумм строк и столбцов $P_{ij}$ - это 1 везде, кроме последних N+1 и M+1 элементов - там стоят M и N соответственно. Также перенесем множители Лагранжа под суммирование по i, j. Тогда запись чуть короче будет:\n",
    "\n",
    "$$ L(P_{ij}[N+1, M+1], l^{u}_{i}[N+1], l^{v}_{j}[M+1]) = \\newline \\sum_{i,j}^{N+1, M+1}  \n",
    " [ P_{ij} \\hat{S_{ij}} + P_{ij} (log(P_{ij}) - 1) + l^{u}_{i} (P_{ij} - \\mu_{i}) + l^{v}_{j} (P_{ij} - \\nu_{j}) ]\n",
    "$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee6ab10",
   "metadata": {
    "cellId": "y16pv9tpd44bey6n6dis6"
   },
   "source": [
    "## Вывод уравнений для итераций\n",
    "#### Производная по $P_{ij}$\n",
    "Необходимым условием экстремума является равенство нулю каждой производной $dL/P_{ij}$, а также равенства нулю производных $dL/dl^{u}_{i}$, $dL/dl^{v}_{j}$.\n",
    "Производные по множителям Лагранжа дадут снова исходные ограничения c $\\mu$ и $\\nu$.\n",
    "\n",
    "Производная по $P_{ij}$:\n",
    "$$\n",
    "dL/dP_{ij} = [ \\hat{S_{ij}} + (log(P_{ij}) - 1) + P_{ij} / P_{ij} + l^{u}_{i} + l^{v}_{j} ] = \\newline\n",
    " = \\hat{S_{ij}} + log(P_{ij}) + l^{u}_{i} + l^{v}_{j}\n",
    "$$\n",
    "Приравниваем 0, выражаем log(P)\n",
    "\n",
    "$$\n",
    "log(P_{ij}) = -\\hat{S_{ij}} - l^{u}_{i} - l^{v}_{j}\n",
    "$$\n",
    "В таком решении получилось, что чем **больше** $S_{ij}$, тем **меньше** $P_{ij}$, что нелогично - мы же максимизируем сумму, если две точки схожи по S - вероятность матча высокая. Метод Лагранжа ищет не максимум или минимум, а все точки экстремума, которые не меняются, если домножить на -1. Обычно этого не делается, т.к. решается задача минимизации стоимости, а не максимизации схожести.\n",
    "\n",
    "Чтобы решение имело более \"интуитивный\" смысл - домножим (мысленно) в исходном лагранжиане первую часть на -1, тогда получится почти такой же результат с точностью до знака при S.\n",
    "Для множителей Лагранжа знак не имеет значения (с точностью до обозначения это одно и то же). Чтобы получить те же знаки, что и у авторов в коде - \"инвертируем\"(домножим на -1) и их тоже. \n",
    "$$\n",
    "log(P_{ij}) = \\hat{S_{ij}} + l^{u}_{i} + l^{v}_{j}\n",
    "$$\n",
    "\n",
    "В довесок еще обозначим $ Z_{ij} = log(P_{ij}) $ (т.е. $P_{ij} = exp(Z_{ij})$). Используем такую замену (по анологии с кодом) по двум причинам:\n",
    "1. В логарифмическом представлении стабильнее вычисления\n",
    "1. В коде в качестве лосса используется кросс-энтропия, которая принимает на вход не вероятности (значения от 0 до 1), а **логиты**(значения от -inf до 0) \n",
    "\n",
    "<font color='#dd33dd'>\n",
    "$$\n",
    "Z_{ij} =  log(P_{ij}) = \\hat{S_{ij}} + l^{u}_{i} + l^{v}_{j}\n",
    "$$\n",
    "</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063af1f4",
   "metadata": {
    "cellId": "cmtj5yj2frgdj1ey72yqx"
   },
   "source": [
    "#### Производная по $l^u, l^v$\n",
    "Производные по $l^u, l^v$ тоже должны быть равны 0, они просто дают исходные ограничения (при вычислении надо правильно учесть внешнюю сумму, лучше видно из длинной записи Лагранжиана):\n",
    "\n",
    "$dL/dl^{u}_i = \\sum_{j}^{M+1}P_{ij} - \\mu_i = 0$\n",
    "\n",
    "$dL/dl^{v}_j = \\sum_{i}^{N+1}P_{ij} - \\nu_i = 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2744c981",
   "metadata": {
    "cellId": "pd9f1u3ehhsd4kfltrmrq"
   },
   "source": [
    "#### Log-space\n",
    "Прологарифмируем $dL/dl^{u}_i$, и используем замену $P_{ij} = exp(Z_{ij}) $.\n",
    "\n",
    "$$\n",
    "log ( \\sum_{j}^{M+1}exp(Z_{ij})) = log(\\mu_i) \\newline\n",
    "$$\n",
    "\n",
    "$$\n",
    "log ( \\sum_{j}^{M+1}exp(\\hat{S_{ij}} + l^{u}_{i} + l^{v}_{j})) = log(\\mu_i) \\newline\n",
    "$$\n",
    "\n",
    "$$\n",
    "log ( \\sum_{j}^{M+1}exp(\\hat{S_{ij}})\\cdot exp(l^{u}_{i})\\cdot exp(l^{v}_{j}))) = log(\\mu_i)\n",
    "$$\n",
    "\n",
    "Суммирование здесь только по j, часть с i можно вынести из под суммы:\n",
    "$$\n",
    "log (exp(l^{u}_{i}) \\sum_{j}^{M+1}exp(\\hat{S_{ij}})\\cdot exp(l^{v}_{j}))) = log(\\mu_i)\n",
    "$$\n",
    "По свойству логарифма log(ab) = log(a) + log(b), log(exp(x)) = x:\n",
    "$$\n",
    "l^{u}_{i} + log(\\sum_{j}^{M+1}exp(\\hat{S_{ij}})\\cdot exp(l^{v}_{j}))) = log(\\mu_i) \\newline\n",
    "$$\n",
    "\n",
    "\n",
    "<font color='#33dd33'>\n",
    "$$\n",
    "l^{u}_{i} = log(\\mu_i) - log(\\sum_{j}^{M+1}exp(\\hat{S_{ij}} + l^{v}_{j}))) \n",
    "$$\n",
    "</font> \n",
    "Точно также для $l^{v}_{j}$:\n",
    "\n",
    "<font color='#33dd33'>\n",
    "$$\n",
    "l^{v}_{j} = log(\\nu_j) - log(\\sum_{i}^{N+1}exp(\\hat{S_{ij}} + l^{u}_{i}))) \n",
    "$$\n",
    "</font> \n",
    "\n",
    "# Зеленые уравнения выше - Sinkhorn Iterations в log-space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f97b2eff",
   "metadata": {
    "cellId": "y67hcaayihiny4gaxxg49s"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\"\"\"\n",
    "Вызов в модуле:\n",
    "\n",
    "        scores = torch.einsum('bdn,bdm->bnm', mdesc0, mdesc1)\n",
    "        scores = scores / self.config['descriptor_dim']**.5\n",
    "\n",
    "        # Run the optimal transport.\n",
    "        scores = log_optimal_transport(\n",
    "            scores, self.bin_score,\n",
    "            iters=self.config['sinkhorn_iterations'])\n",
    "\"\"\"\n",
    "\n",
    "def log_optimal_transport(scores, alpha, iters: int):\n",
    "    \"\"\" Perform Differentiable Optimal Transport in Log-space for stability\"\"\"\n",
    "    # scores - матрица S[B, N, M]\n",
    "    b, m, n = scores.shape # [B, N, M]\n",
    "    one = scores.new_tensor(1) # scalar 1\n",
    "    ms, ns = (m*one).to(scores), (n*one).to(scores) # scalar N, scalar M \n",
    "    # dustbins: alpha x shape\n",
    "    bins0 = alpha.expand(b, m, 1) \n",
    "    bins1 = alpha.expand(b, 1, n)\n",
    "    alpha = alpha.expand(b, 1, 1)\n",
    "    \n",
    "    # [B, N + 1, M + 1], она же S с крышкой\n",
    "    couplings = torch.cat([torch.cat([scores, bins0], -1),\n",
    "                           torch.cat([bins1, alpha], -1)], 1)\n",
    "    \n",
    "    # Как я понял, здесь norm - это еще один трюк для стабилизации.\n",
    "    # norm = -log(M+N), и сначала он добавляется в log_mu, log_nu,\n",
    "    # а потом вычитается из Z.\n",
    "    # Я протестировал, что если заменить norm на log(1), т.е. на 0\n",
    "    # то все продолжает работать\n",
    "    norm = - (ms + ns).log() # можно заменить на one.log()\n",
    "    \n",
    "    # [ -log(M+N) ... x M ... -log(M+N), log(M) - log(M+N)] [M+1] \n",
    "    # В последний элемент прибавляется log(M) или log(N)\n",
    "    # Если norm=log(1) = 0, то вектор равен\n",
    "    # [log(1), log(1) ..., log(M)]\n",
    "    log_mu = torch.cat([norm.expand(m), ns.log()[None] + norm])\n",
    "    log_nu = torch.cat([norm.expand(n), ms.log()[None] + norm])\n",
    "        \n",
    "    # бродкаст на все батчи\n",
    "    log_mu, log_nu = log_mu[None].expand(b, -1), log_nu[None].expand(b, -1)\n",
    "    # выполняем iters итераций, получаем логарифм(!) вероятности матча\n",
    "    Z = log_sinkhorn_iterations(couplings, log_mu, log_nu, iters)\n",
    "    # откатываем norm обратно (вычитаем -log(M+N))\n",
    "    Z = Z - norm  # multiply probabilities by M+N\n",
    "    return Z\n",
    "\n",
    "def log_sinkhorn_iterations(Z, log_mu, log_nu, iters: int):\n",
    "    \"\"\" Perform Sinkhorn Normalization in Log-space for stability\"\"\"\n",
    "    # инициализируем нулями u,v множители Лагранжа\n",
    "    u, v = torch.zeros_like(log_mu), torch.zeros_like(log_nu)\n",
    "    for _ in range(iters):\n",
    "        # torch.logsumexp = torch.log(torch.sum(torch.exp(...)))\n",
    "        u = log_mu - torch.logsumexp(Z + v.unsqueeze(1), dim=2)\n",
    "        v = log_nu - torch.logsumexp(Z + u.unsqueeze(2), dim=1)\n",
    "    return Z + u.unsqueeze(2) + v.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a3599b",
   "metadata": {
    "cellId": "5ebjqrngx0ux9jdcmsgles"
   },
   "source": [
    "## Тесты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e45d26a6",
   "metadata": {
    "cellId": "fusaa1w8ay97d2dzilm58g",
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = (torch.randn((1, 3, 5)) - 0.5) * 2\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c20b6363",
   "metadata": {
    "cellId": "5hoc31oddkipz3tzd3e7"
   },
   "outputs": [],
   "source": [
    "a = torch.Tensor([0.00042])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08d5f463",
   "metadata": {
    "cellId": "3786kls3anl2yzb2yf79qg"
   },
   "outputs": [],
   "source": [
    "torch.round(torch.exp(log_optimal_transport(t, a, 2)) * 100).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82c37395",
   "metadata": {
    "cellId": "4jx91fas1zgjnz37rgent"
   },
   "outputs": [],
   "source": [
    "# сумма по 1 оси\n",
    "# Сумма dustbin = N\n",
    "torch.exp( # Матрица возводится в экспоненту!\n",
    "    log_optimal_transport(t, a, 2)\n",
    ").numpy().round(2).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ef49461f",
   "metadata": {
    "cellId": "dfpd8hcmw6w4ww4qq41on"
   },
   "outputs": [],
   "source": [
    "# сумма по 2 оси\n",
    "# Сумма dustbin = M\n",
    "(torch.exp(log_optimal_transport(t, a, 2))).numpy().round(2).sum(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "57d6eade",
   "metadata": {
    "cellId": "pz8f9e84trkqpngawfw1"
   },
   "outputs": [],
   "source": [
    "# Сумма всей матрицы\n",
    "# M + N\n",
    "(torch.exp(log_optimal_transport(t, a, 2))).numpy().round(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3fd2c196",
   "metadata": {
    "cellId": "kfa0q1eqkijz2m2skuw7j"
   },
   "outputs": [],
   "source": [
    "# Результаты при 1 итерации\n",
    "torch.exp(log_optimal_transport(t, a, 1)).numpy().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f9ec7062",
   "metadata": {
    "cellId": "l2zp1cuykzwecr79hyahp"
   },
   "outputs": [],
   "source": [
    "# Результаты при 3 итерациях (разница большая с iters=1)\n",
    "torch.exp(log_optimal_transport(t, a, 3)).numpy().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "37d96476",
   "metadata": {
    "cellId": "xaj21r833omfhhmdkig4bl"
   },
   "outputs": [],
   "source": [
    "# Результаты при 10 итерациях (разница не оч большая с iters=3)\n",
    "torch.exp(log_optimal_transport(t, a, 10)).numpy().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7f5501",
   "metadata": {
    "cellId": "pbxpn97p425u4cqaa39hw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f8a0b1",
   "metadata": {
    "cellId": "mujuzw6e1thu0wn2da164"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "1ab5d77e-2eab-4b71-bc1b-b67ba0578e8a",
  "notebookPath": "course_cvdl/classes/c08/SinkhornIterations.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

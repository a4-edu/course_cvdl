{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2220571-43fc-4ff0-adac-5973b1312ae2",
   "metadata": {},
   "source": [
    "## Занятие 10. Распознавание действий на видео\n",
    "\n",
    "### Часть 1. Пакеты Open-MMLab\n",
    "\n",
    "Фреймворк open-mmlab - набор пакетов для решения задач компьютерного зрения от Multimedia Lab в The Chinese University of Hong Kong.\n",
    "- https://github.com/open-mmlab/\n",
    "- https://openmmlab.com/\n",
    "\n",
    "Наиболее известный пакет фреймворка - [mmdetection](https://github.com/open-mmlab/mmdetection)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1142dd5e-6d46-46ed-91df-ba62e2a19228",
   "metadata": {},
   "source": [
    "Пакеты open-mmlab не являются Python-пакетами в стандартном смысле - помимо Python-кода моделей и чтения данных, в него входят скрипты для обучения, распределенного обучения, тестирования, скачивания данных, конвертации моделей, анализу качества работы и производительности\n",
    "\n",
    "Авторы стремятся в работе:\n",
    "- к воспроизводимости и документированности результатов\n",
    "- к модульности моделей, расширяемости фреймворка новыми моделями\n",
    "- к самодостаточности фреймворка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77427335-5917-4ada-bcb7-1dbcd641037e",
   "metadata": {},
   "source": [
    "#### 1.1 Конфиги\n",
    "Для задания пайплайнов используются python-модули с описанием \"всего\" в виде словарей.\n",
    "\n",
    "\n",
    "Пример: \n",
    "- пайплайн CenterNet https://github.com/open-mmlab/mmdetection/blob/master/configs/centernet/README.md\n",
    "- код головы CenterNet https://github.com/open-mmlab/mmdetection/blob/master/mmdet/models/dense_heads/centernet_head.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd42c32-f76b-432b-aef9-8fe6deecf7c6",
   "metadata": {},
   "source": [
    "### Часть 2. MMAction2 и анализ видео\n",
    "https://github.com/open-mmlab/mmaction2 - фреймворк для анализа видео (распознавания действий на видео).\n",
    "\n",
    "Распознавание действий на видео (Action Recognition / Video Understanding) - общая задача со множеством вариаций.\n",
    "\n",
    "Постановка задачи зависит от определения \"действия\", например:\n",
    "1. У действия есть границы во времени ($T_{start}, T_{end}$) - или оно длится от начала до конца видео?\n",
    "2. Действие на видео одно - или их много?\n",
    "3. Действие имеет пространственные характеристики (Bounding Box, Mask) - или оно происходит \"на всём кадре целиком\"?\n",
    "4. Действия могут пересекаться во времени / в пространстве?\n",
    "\n",
    "...\n",
    "\n",
    "Дальше - больше:\n",
    "- действие описывается единственным фиксированным классом (Dance), несколькими классами (Actor / Action), текстом в свободной форме?\n",
    "- есть ли другие модальности - например, аудио?\n",
    "- камера одна или несколько?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e07a66-ba2d-4124-87b8-ef01c1ea6557",
   "metadata": {},
   "source": [
    "### Примеры задач\n",
    "\n",
    "- Action Recognition: на видео - одно действие фиксированного класса без границ по времени и пространству\n",
    "- Activity Localization: на видео одно или несколько действий фиксированных классов с границами по времени\n",
    "- Spatio-Temporal Action Detection = Action Localization + детекции объектов\n",
    "- Action Segmentation: на каждом кадре видео не более одного действия\n",
    "- Action (Event) Spotting: на видео есть отдельные кадры-действия (у действия нет длительности)\n",
    "\n",
    "Часть задач поддерживается [mmaction2](https://github.com/open-mmlab/mmaction2#supported-methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c00c3c-16a5-475e-b769-dfcbe1538900",
   "metadata": {},
   "source": [
    "### Часть 3.1. Пример использования обученных моделей\n",
    "Запустим предобученную spatio-temporal action detection на двух примерах.\n",
    "- [demo](https://github.com/open-mmlab/mmaction2/tree/master/demo#spatiotemporal-action-detection-video-demo)\n",
    "- [config](https://github.com/open-mmlab/mmaction2/tree/main/configs/detection/slowonly/slowonly_kinetics400-pretrained-r101_8xb16-8x8x1-20e_ava21-rgb.py)\n",
    "\n",
    "Для запуска демо нужно:\n",
    "- `$ pip install mmdet`\n",
    "- `$ pip install moviepy`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65542817-a9a2-4442-872a-5122686f6ab0",
   "metadata": {},
   "source": [
    "```\n",
    "(open-mmlab) python mmaction2/demo/demo_spatiotemporal_det.py `\r\n",
    "F:\\bzimka\\edu\\course_cvdl\\classes\\c10\\data\\ntu_sample.avi F:\\bzimka\\edu\\course_cvdl\\classes\\c10\\data\\ntu_spatiotemp.mp4 `\r\n",
    "    --config mmaction2/configs/detection/slowonly/slowonly_kinetics400-pretrained-r101_8xb16-8x8x1-20e_ava21-rgb.py `\r\n",
    "    --checkpoint https://download.openmmlab.com/mmaction/detection/ava/slowonly_omnisource_pretrained_r101_8x8x1_20e_ava_rgb/slowonly_omnisource_pretrained_r101_8x8x1_20e_ava_rgb_20201217-16378594.pth `\r\n",
    "    --det-config mmaction2/demo/demo_configs/faster-rcnn_r50_fpn_2x_coco_infer.py `\r\n",
    "    --det-checkpoint http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth `\r\n",
    "    --det-score-thr 0.9 `\r\n",
    "    --action-score-thr 0.5 `\r\n",
    "    --label-map mmaction2/tools/data/ava/label_map.txt `\r\n",
    "    --predict-stepsize 8 `\r\n",
    "    --output-stepsize 4 `\r\n",
    "    --output-fps 6 `\r\n",
    "    --device cpu:0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52d1b34b-9879-4eec-bd7d-4133756a8aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"data/ntu_spatiotemp.mp4\" controls  width=\"1024\"  height=\"576\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"data/ntu_spatiotemp.mp4\", width=1024, height=576)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a951ab0c-dcf9-460c-a93b-5ebb808b87dc",
   "metadata": {},
   "source": [
    "```\n",
    "(openmmlab) PS F:\\bzimka\\edu\\course_cvdl\\classes\\c10> python mmaction2/demo/demo_spatiotemporal_det.py `\r\n",
    "     data\\losash_tem.mp4 data\\losash_tem_spatiotemp.mp4 `\r\n",
    "     --config mmaction2/configs/detection/slowonly/slowonly_kinetics400-pretrained-r101_8xb16-8x8x1-20e_ava21-rgb.py `\r\n",
    "     --checkpoint https://download.openmmlab.com/mmaction/detection/ava/slowonly_omnisource_pretrained_r101_8x8x1_20e_ava_rgb/slowonly_omnisource_pretrained_r101_8x8x1_20e_ava_rgb_20201217-16378594.pth `\r\n",
    "     --det-config mmaction2/demo/demo_configs/faster-rcnn_r50_fpn_2x_coco_infer.py `\r\n",
    "     --det-checkpoint http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth `\r\n",
    "     --det-score-thr 0.1 `\r\n",
    "     --action-score-thr 0.1 `\r\n",
    "     --label-map mmaction2/tools/data/ava/label_map.txt `\r\n",
    "     --predict-stepsize 8 `\r\n",
    "     --output-stepsize 4 `\r\n",
    "     --output-fps 6 `\r\n",
    "     --device cuda:0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "119bc746-c8f1-4ccb-904f-b1796c858aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"data/losash_tem_spatiotemp.mp4\" controls  width=\"1024\"  height=\"576\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"data/losash_tem_spatiotemp.mp4\", width=1024, height=576)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbe047a-e4e6-4ea7-b508-3e13b8ead4af",
   "metadata": {},
   "source": [
    "### Часть 4. Пример тренировки и тестирования модели\n",
    "\n",
    "В качестве примера для обучения возьмём другую задачу - Action Localization c Boundary Matching Network и воспроизведем ее результаты на датасете [ActivityNet](https://github.com/open-mmlab/mmaction2/blob/master/tools/data/activitynet/README.md).\n",
    "\n",
    "[config](https://github.com/open-mmlab/mmaction2/blob/master/configs/localization/bmn/README.md)\n",
    "\n",
    "Сеть предсказывает proposals (сегменты) для отдельных действий."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104d100f-7c98-446b-b037-c43b1aa22dd0",
   "metadata": {},
   "source": [
    "#### 4.1. Подготовка датасета ActivityNet\n",
    "Первые two-stage детекторы (R-CNN) работали по принципу:\n",
    "- обучить классификатор изображений\n",
    "- применить классификатор к патчам изображения, сохранить признаки с последнего слоя\n",
    "- обучить на признаках сеть-детектор (Region Proposal Network)\n",
    "\n",
    "Аналогично работает BMN:\n",
    "- обучить классификатор изображений либо видео (2D/3D CNN)\n",
    "- применить классификатор к последовательности кадров, сохранить признаки с последнего слоя\n",
    "- обучить на признаках сеть-детектор (Region Proposal Network)\n",
    "\n",
    "\n",
    "Будем тренировать только последний этап - [воспользуемся](https://github.com/open-mmlab/mmaction2/blob/master/tools/data/activitynet/README.md#option-1-use-the-activitynet-rescaled-feature-provided-in-this-repo) готовыми фичами:\n",
    "```\n",
    "$ cd mmaction2/tools/data\n",
    "$ bash download_feature_annotations.sh\n",
    "$ bash download_features.sh\n",
    "$ python process_annotations.py\n",
    "```\n",
    "Фичи каждого видео - тензор \\[400, T\\], приведенные усреднением к \\[400, 100\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41723b44-e972-461e-9171-0e5b634e07cd",
   "metadata": {},
   "source": [
    "#### 4.2. Обучение BMN на ActivityNet\n",
    "`$ python .\\tools\\train.py .\\configs\\localization\\bmn\\bmn_2xb8-400x100-9e_activitynet-feature.py`\n",
    "\n",
    "Результаты тренировки автоматически сохраняются в work_dirs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658b0a87-6d36-4c09-87b5-61336e4367b6",
   "metadata": {},
   "source": [
    "#### 4.3. Тестирование BMN на ActivityNet\n",
    "Правки:\n",
    "- добавить gpu_ids в config\n",
    "\n",
    "`$ python tools/test.py configs/localization/bmn/bmn_400x100_2x8_9e_activitynet_feature.py .\\work_dirs\\bmn_400x100_2x8_9e_activitynet_feature\\latest.pth --eval AR@AN --out .\\work_dirs\\bmn_400x100_2x8_9e_activitynet_feature\\results_test.json --gpu-collect`\n",
    "\n",
    "```\n",
    "auc: 66.9812\n",
    "AR@1: 0.3360\n",
    "AR@5: 0.4958\n",
    "AR@10: 0.5620\n",
    "AR@100: 0.7503\n",
    "```\n",
    "Сравнить с [config](https://github.com/open-mmlab/mmaction2/blob/master/configs/localization/bmn/README.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ab69b0-45ff-4f5c-9a08-f70c806df0a8",
   "metadata": {},
   "source": [
    "### Часть 5. Собственные расширения фреймворка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1ae2f2d-cb27-4cc7-a6f0-b9f3d77e36b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmaction.models.localizers import BMN\n",
    "from mmaction.registry import MODELS\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54252816-bb41-4ab9-b8e2-94c2277c55fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config mmaction2\\configs\\_base_\\models\\bmn_400x100.py\n",
    "\n",
    "model_config = dict(\n",
    "    type='BMN',\n",
    "    temporal_dim=100,\n",
    "    boundary_ratio=0.5,\n",
    "    num_samples=32,\n",
    "    num_samples_per_bin=3,\n",
    "    feat_dim=400,\n",
    "    soft_nms_alpha=0.4,\n",
    "    soft_nms_low_threshold=0.5,\n",
    "    soft_nms_high_threshold=0.9,\n",
    "    post_process_top_k=100\n",
    ")\n",
    "model = MODELS.build(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fd6c2b7-56fb-476e-90a3-71b4f8c87847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BMN(\n",
       "  (data_preprocessor): BaseDataPreprocessor()\n",
       "  (loss_cls): BMNLoss()\n",
       "  (x_1d_b): Sequential(\n",
       "    (0): Conv1d(400, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=4)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=4)\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (x_1d_s): Sequential(\n",
       "    (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=4)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv1d(256, 1, kernel_size=(1,), stride=(1,))\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       "  (x_1d_e): Sequential(\n",
       "    (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=4)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv1d(256, 1, kernel_size=(1,), stride=(1,))\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       "  (x_1d_p): Sequential(\n",
       "    (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (x_3d_p): Sequential(\n",
       "    (0): Conv3d(256, 512, kernel_size=(32, 1, 1), stride=(1, 1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (x_2d_p): Sequential(\n",
       "    (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(128, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (7): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b3e31ab-3cc2-4c56-bc7a-a74a77a531ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config.pop('type');\n",
    "model = BMN(**model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41b86f66-8d51-4e37-ab77-48c6de974260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num params: 36.979 M\n"
     ]
    }
   ],
   "source": [
    "print(f\"Num params: {sum(p.numel() for (_, p) in model.named_parameters()) /1e6:1.3f} M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3780f89b-dbed-447a-888d-d8196c3e947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_raw_feature = torch.rand(400, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40dd7846-3f5d-459e-9602-65ef47b5a8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: mmaction2\\work_dirs\\bmn_400x100_2x8_9e_activitynet_feature\\epoch_9.pth\n"
     ]
    }
   ],
   "source": [
    "from mmengine.runner import load_checkpoint\n",
    "from pathlib import Path\n",
    "load_checkpoint(model, str(Path() / \"mmaction2\" /\"work_dirs\"/\"bmn_400x100_2x8_9e_activitynet_feature\" / \"epoch_9.pth\"), map_location='cpu');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34bee6df-c381-455e-9957-ffa1a9f64bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = ActionDataSample(\n",
    "    metainfo=dict(\n",
    "        video_name='v_test',\n",
    "        duration_second=100,\n",
    "        duration_frame=960,\n",
    "        feature_frame=960\n",
    "    )\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    results = model([one_raw_feature], mode=\"predict\", data_samples=[sample])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e9b9a65-3117-476c-831b-ab464cdb346e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'video_name': 'v_test',\n",
       "  'proposal_list': [{'score': 0.12671509385108948, 'segment': [0.0, 99.0]},\n",
       "   {'score': 0.0693671852350235, 'segment': [0.0, 88.0]},\n",
       "   {'score': 0.05580621957778931, 'segment': [11.0, 99.0]},\n",
       "   {'score': 0.03480414301156998, 'segment': [26.0, 99.0]},\n",
       "   {'score': 0.02557065337896347, 'segment': [0.0, 74.0]},\n",
       "   {'score': 0.023259269073605537, 'segment': [15.0, 88.0]},\n",
       "   {'score': 0.0068535394966602325, 'segment': [0.0, 51.0]},\n",
       "   {'score': 0.00631954800337553, 'segment': [0.0, 30.0]},\n",
       "   {'score': 0.00492625730112195, 'segment': [0.0, 14.000000000000002]},\n",
       "   {'score': 0.0045216986909508705, 'segment': [42.0, 99.0]},\n",
       "   {'score': 0.003462002146989107, 'segment': [60.0, 99.0]},\n",
       "   {'score': 0.0034392739180475473, 'segment': [0.0, 7.000000000000001]},\n",
       "   {'score': 0.0028525996021926403, 'segment': [74.0, 99.0]},\n",
       "   {'score': 0.0027622701169414928, 'segment': [11.0, 84.0]},\n",
       "   {'score': 0.002292781136929989, 'segment': [28.999999999999996, 86.0]},\n",
       "   {'score': 0.0022409106807388204, 'segment': [1.0, 75.0]},\n",
       "   {'score': 0.002032736549153924, 'segment': [18.0, 75.0]},\n",
       "   {'score': 0.0015768454177305102, 'segment': [87.0, 99.0]},\n",
       "   {'score': 0.0015367405721917748, 'segment': [8.0, 66.0]},\n",
       "   {'score': 0.0014925741124898195, 'segment': [0.0, 2.0]},\n",
       "   {'score': 0.0014439952792599797, 'segment': [0.0, 1.0]},\n",
       "   {'score': 0.0013491555582731962, 'segment': [50.0, 91.0]},\n",
       "   {'score': 0.0009948964368263116, 'segment': [0.0, 18.0]},\n",
       "   {'score': 0.0009246282279491425, 'segment': [6.0, 40.0]},\n",
       "   {'score': 0.0008111237540921983, 'segment': [0.0, 4.0]},\n",
       "   {'score': 0.0007802326491739024, 'segment': [0.0, 95.0]},\n",
       "   {'score': 0.0007295811803920627, 'segment': [0.0, 56.00000000000001]},\n",
       "   {'score': 0.000723149802070111, 'segment': [95.0, 99.0]},\n",
       "   {'score': 0.0006594886654056609, 'segment': [15.0, 56.99999999999999]},\n",
       "   {'score': 0.0005636853748001158, 'segment': [65.0, 90.0]},\n",
       "   {'score': 0.0005388511344790459, 'segment': [39.0, 45.0]},\n",
       "   {'score': 0.0005147385527379811, 'segment': [39.0, 42.0]},\n",
       "   {'score': 0.0005094492225907743, 'segment': [82.0, 96.0]},\n",
       "   {'score': 0.0004833679413422942, 'segment': [39.0, 80.0]},\n",
       "   {'score': 0.0004737303825095296, 'segment': [74.0, 88.0]},\n",
       "   {'score': 0.00047260953579097986, 'segment': [74.0, 77.0]},\n",
       "   {'score': 0.0004533985920716077, 'segment': [38.0, 41.0]},\n",
       "   {'score': 0.0004511197912506759, 'segment': [11.0, 14.000000000000002]},\n",
       "   {'score': 0.00044962833635509014, 'segment': [39.0, 51.0]},\n",
       "   {'score': 0.0004438025353010744, 'segment': [92.0, 98.0]},\n",
       "   {'score': 0.00044096328201703727, 'segment': [50.0, 76.0]},\n",
       "   {'score': 0.0004403134516905993, 'segment': [42.0, 45.0]},\n",
       "   {'score': 0.00043934021960012615, 'segment': [39.0, 66.0]},\n",
       "   {'score': 0.00043578926124610007, 'segment': [86.0, 89.0]},\n",
       "   {'score': 0.0004151384928263724, 'segment': [6.0, 24.0]},\n",
       "   {'score': 0.00041424119262956083, 'segment': [11.0, 32.0]},\n",
       "   {'score': 0.00040672547766007483, 'segment': [42.0, 57.99999999999999]},\n",
       "   {'score': 0.0004028341791126877, 'segment': [87.0, 90.0]},\n",
       "   {'score': 0.0003981025074608624, 'segment': [82.0, 88.0]},\n",
       "   {'score': 0.00039640229078941047, 'segment': [49.0, 52.0]},\n",
       "   {'score': 0.000395823793951422, 'segment': [56.00000000000001, 70.0]},\n",
       "   {'score': 0.00038559635868296027, 'segment': [15.0, 22.0]},\n",
       "   {'score': 0.00038484076503664255, 'segment': [43.0, 46.0]},\n",
       "   {'score': 0.00038386412779800594, 'segment': [49.0, 65.0]},\n",
       "   {'score': 0.000383614533348009, 'segment': [42.0, 48.0]},\n",
       "   {'score': 0.00038089690497145057, 'segment': [65.0, 70.0]},\n",
       "   {'score': 0.0003779603575821966, 'segment': [35.0, 44.0]},\n",
       "   {'score': 0.00037163420347496867, 'segment': [50.0, 53.0]},\n",
       "   {'score': 0.00036950333742424846, 'segment': [28.999999999999996, 51.0]},\n",
       "   {'score': 0.00036584760528057814, 'segment': [71.0, 76.0]},\n",
       "   {'score': 0.00036398012889549136, 'segment': [74.0, 80.0]},\n",
       "   {'score': 0.00036252744030207396, 'segment': [8.0, 14.000000000000002]},\n",
       "   {'score': 0.00036204620846547186, 'segment': [11.0, 20.0]},\n",
       "   {'score': 0.0003572220157366246, 'segment': [18.0, 37.0]},\n",
       "   {'score': 0.00035658793058246374, 'segment': [50.0, 59.0]},\n",
       "   {'score': 0.0003556065203156322, 'segment': [18.0, 21.0]},\n",
       "   {'score': 0.00035525017301552, 'segment': [31.0, 41.0]},\n",
       "   {'score': 0.0003548723761923611, 'segment': [31.0, 36.0]},\n",
       "   {'score': 0.0003546272637322545, 'segment': [1.0, 3.0]},\n",
       "   {'score': 0.0003535035066306591, 'segment': [35.0, 38.0]},\n",
       "   {'score': 0.00035318901063874364, 'segment': [26.0, 33.0]},\n",
       "   {'score': 0.00035273144021630287, 'segment': [65.0, 76.0]},\n",
       "   {'score': 0.00035250485690919384, 'segment': [0.0, 10.0]},\n",
       "   {'score': 0.00034982041688635945, 'segment': [62.0, 66.0]},\n",
       "   {'score': 0.0003439309075474739, 'segment': [56.00000000000001, 63.0]},\n",
       "   {'score': 0.00033997464925050735, 'segment': [69.0, 72.0]},\n",
       "   {'score': 0.00033845024881884456, 'segment': [62.0, 72.0]},\n",
       "   {'score': 0.00033774759504012764, 'segment': [6.0, 9.0]},\n",
       "   {'score': 0.00033550916123203933, 'segment': [28.999999999999996, 32.0]},\n",
       "   {'score': 0.0003297968942206353, 'segment': [56.00000000000001, 59.0]},\n",
       "   {'score': 0.0003268020518589765, 'segment': [18.0, 27.0]},\n",
       "   {'score': 0.0003263908423389503, 'segment': [26.0, 95.0]},\n",
       "   {'score': 0.0003253322502132505, 'segment': [23.0, 27.0]},\n",
       "   {'score': 0.00032439662027172744, 'segment': [23.0, 31.0]},\n",
       "   {'score': 0.0003231787122786045, 'segment': [23.0, 45.0]},\n",
       "   {'score': 0.00032100308453664184, 'segment': [60.0, 64.0]},\n",
       "   {'score': 0.0003184696251992136, 'segment': [8.0, 11.0]},\n",
       "   {'score': 0.0003181876672897488, 'segment': [6.0, 12.0]},\n",
       "   {'score': 0.000311808631522581, 'segment': [80.0, 84.0]},\n",
       "   {'score': 0.0003100246249232441, 'segment': [60.0, 81.0]},\n",
       "   {'score': 0.0003091798280365765, 'segment': [47.0, 51.0]},\n",
       "   {'score': 0.00030848290771245956, 'segment': [78.0, 92.0]},\n",
       "   {'score': 0.00030774716287851334, 'segment': [26.0, 28.999999999999996]},\n",
       "   {'score': 0.00030764960683882236, 'segment': [49.0, 55.00000000000001]},\n",
       "   {'score': 0.0003069003578275442, 'segment': [39.0, 40.0]},\n",
       "   {'score': 0.0002940595441032201, 'segment': [69.0, 79.0]},\n",
       "   {'score': 0.0002916666562668979, 'segment': [52.0, 56.00000000000001]},\n",
       "   {'score': 0.00028513476718217134, 'segment': [82.0, 85.0]},\n",
       "   {'score': 0.00028299857513047755, 'segment': [86.0, 92.0]},\n",
       "   {'score': 0.00025701531558297575, 'segment': [90.0, 96.0]}]}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a039c91-6985-4721-ba08-48cecac03513",
   "metadata": {},
   "source": [
    "### Напишем модификацию - BMN with Action Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23026be5-7fab-464d-a86a-4a5b8a429df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mmaction2\\mmaction\\models\\localizers\\bmn_w_ac.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mmaction2\\mmaction\\models\\localizers\\bmn_w_ac.py\n",
    "from torch import nn\n",
    "from mmaction.models.localizers import BMN\n",
    "from mmaction.registry import MODELS\n",
    "\n",
    "# https://raw.githubusercontent.com/imatge-upc/activitynet-2016-cvprw/master/dataset/labels.txt\n",
    "NUM_ACTIVITYNET_LABELS = 201\n",
    "\n",
    "@MODELS.register_module()\n",
    "class BMNwActionCls(BMN):\n",
    "    \"\"\"\n",
    "    BMN with action classification.\n",
    "    Adds head for action classificationю.\n",
    "    <Head training is not implemented>.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.num_action_classes = kwargs.pop('num_action_classes', NUM_ACTIVITYNET_LABELS)\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.head_action_cls = nn.Sequential(\n",
    "            nn.Conv1d(self.feat_dim, self.num_action_classes, kernel_size=1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def predict(self, raw_feature, video_meta):\n",
    "        outputs = super().predict(raw_feature, video_meta)        \n",
    "        action_cls_pred = self.head_action_cls(raw_feature)\n",
    "        # [batch_size, self.num_action_classes, self.tscale]\n",
    "        batch_size, nc, ts = action_cls_pred.shape\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for p, prop in enumerate(outputs[b]['proposal_list']):\n",
    "                t_start, t_end = prop['segment']\n",
    "                proposal_mean_prob = action_cls_pred[b, :, round(t_start):round(t_end)].mean(axis=1)\n",
    "                outputs[b]['proposal_list'][p]['action_cls'] = torch.argmax(proposal_mean_prob)\n",
    "        return outputs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a91f6f61-6c1b-4f2a-a39a-480fac5c4704",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wc = BMNwActionCls(num_action_classes=42, **model_config)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b96f706-9b8b-4d4b-89bf-298cf9c74e45",
   "metadata": {},
   "source": [
    "model_wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31379ee9-4e34-43ec-a184-89aa5992600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    results = model_wc([one_raw_feature], mode=\"predict\", data_samples=[sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb98e025-6500-44a0-9479-88e595d7cf96",
   "metadata": {},
   "source": [
    "### Использование модификации\n",
    "- Добавить файл mmaction/models/localizers/bmn_w_ac.py\n",
    "- Добавить импорт модели в mmaction/models/localizers/\\__init__.py (иначе во время запуска скрипт тренировки не найдёт новую модель)\n",
    "```\n",
    "python .\\tools\\train.py .\\configs\\localization\\bmn\\bmn_400x100_2x8_9e_activitynet_feature.py \n",
    "    --gpus=1 \n",
    "    --work-dir=./work_dirs/bmn_400x100_2x8_9e_activitynet_feature_w_ac/ \n",
    "    --cfg-options model.type=BMNwActionCls \n",
    "    --cfg-options data.videos_per_gpu=4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6a4f9a-da95-403d-a500-23bcdb120420",
   "metadata": {},
   "source": [
    "### Итоги\n",
    "- есть множество вариаций задач распознавания видео\n",
    "- многие из них имеют baseline решения в пакете mmaction2\n",
    "- у open-mmlab также есть ряд пакетов для других задач компьютерного зрения\n",
    "- решения из пакетов open-mmlab легко воспроизвести и можно модифицировать под свои нужды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cee49b-16ae-4f43-ab20-e7df202b2ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
